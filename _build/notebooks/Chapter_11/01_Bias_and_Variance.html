---
redirect_from:
  - "/notebooks/chapter-11/01-bias-and-variance"
interact_link: content/notebooks/Chapter_11/01_Bias_and_Variance.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Bias and Variance
prev_page:
  url: /notebooks/Chapter_11/00_Bias_Variance_and_Least_Squares.html
  title: |-
    Bias, Variance, and Least Squares
next_page:
  url: 
  title: |-
    
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bias-and-Variance">Bias and Variance<a class="anchor-link" href="#Bias-and-Variance"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Suppose we are trying to estimate a constant numerical parameter $\theta$, and our estimator is the statistic $T$. Remember that a statistic is a number that we can calculate from our random sample; since the sample is random, so is the statistic.</p>
<p>To assess the quality of an estimator, we have to examine how its values compare with the parameter being estimated. We will first do a qualitative analysis and then move on to formal definitions.</p>
<p>Each figure below corresponds to a different estimator. In each case, the parameter $\theta$ is at the red dot on the number line. We have generated a few values of the estimator and have plotted them in blue.</p>
<ul>
<li><p>The figure on the top left corresponds to an estimator that is unbiased and has low variance. Its values are tightly clustered around the red dot. This kind of estimator is desirable: it is rarely far from the parameter, and it doesn't systematically overestimate or underestimate.</p>
</li>
<li><p>The figure on the bottom left corresponds to an estimator that has low variance but is <em>biased</em>. We will give a formal definition of bias later in this section; for now, just think of bias as a systematic overestimation or underestimation. The estimator in this figure overestimates by quite a bit compared to the estimators in the other figures.</p>
</li>
<li><p>The figure on the top right corresponds to an estimator that is unbiased but has high variance. The six values plotted do look roughly evenly distributed around the red dot, but some of them are quite far from the red dot, on either side.</p>
</li>
<li><p>The figure on the bottom right corresponds to an estimator that has low variance and also low bias. It's not unbiased â€“ you can see that it overestimates in general. But not by much! And because of its low variance, it is rarely very far from the parameter. So, even though the estimator is biased, we might want to use it because we know its value will be close to the parameter.</p>
</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<table><tr>
<td> <img src="../../images/ubias_lvar.png" alt="unbiased, low variance" style="width: 400px;"/> </td>
<td> <img src="../../images/ubias_hvar.png" alt="unbiased, high variance" style="width: 400px;"/> </td>
</tr></table><table><tr>
<td> <img src="../../images/bias_lvar.png" alt="biased, low variance" style="width: 400px;"/> </td>
<td> <img src="../../images/lbias_lvar.png" alt="low bias, low variance" style="width: 400px;"/> </td>
</tr></table>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's attempt a quantitative analysis of what we have seen.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Mean-Squared-Error">Mean Squared Error<a class="anchor-link" href="#Mean-Squared-Error"> </a></h3><p>The error in our estimate is $T - \theta$. The <em>mean squared error</em> of the estimator $T$ is defined as</p>
<p>$$
MSE_\theta(T) ~ = ~ E_\theta\big{(}(T - \theta)^2\big{)}
$$</p>
<p>We are using $\theta$ as a subscript to remind us that the expectation is calculated under the assumption that $\theta$ is the true value of the parameter.</p>
<p>The mean squared error can be used to assess the quality of an estimator: lower is better.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decomposition-of-Error">Decomposition of Error<a class="anchor-link" href="#Decomposition-of-Error"> </a></h3><p>Let's try and understand the reasons why there is error in our estimate.</p>
<p>The first reason is that values of $T$ depend on the sample and can therefore vary. The expected value is $E_\theta(T)$, but $T$ will typically differ from that. The difference is the familiar deviation</p>
<p>$$
D_\theta(T) ~ = ~ T - E_\theta(T)
$$</p>
<p>The deviation $D_\theta(T)$ is a random variable.</p>
<p>The second reason for the error is bias, which we will now define formally.</p>
<p>Recall that in an earlier chapter we defined $T$ to be an unbiased estimator of $\theta$ if $E_\theta(T) = \theta$. But estimators can be biased. The <em>bias</em> of the estimator $T$ is defined to be the amount by which its expectation exceeds the parameter:</p>
<p>$$
B_\theta(T) ~ = ~ E_\theta(T) - \theta
$$</p>
<p>Both $E_\theta(T)$ and $\theta$ are constants, so the bias $B_\theta(T)$ is a constant. It's not a random variable.</p>
<p>If the bias is positive, then $T$ tends to overestimate $\theta$. If the bias is negative, then $T$ tends to underestimate $\theta$.</p>
<p>We now have a <strong>decomposition of the error</strong> as the sum of the deviation and the bias:</p>
<p>$$
\begin{align*}
T - \theta ~ &amp;= ~ \big{(}T - E_\theta(T)\big{)} + \big{(} E_\theta(T) - \theta \big{)} \\
&amp;= ~ D_\theta(T) + B_\theta(T)
\end{align*}
$$</p>
<p>The figure below illustrates the decomposition. The green segment represents the deviation and the red segment is the bias.</p>
<p><img src="../../images/error_decomp.png" alt="error decomposition"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bias-Variance-Decomposition">Bias-Variance Decomposition<a class="anchor-link" href="#Bias-Variance-Decomposition"> </a></h3><p>This leads to a decomposition of the mean squared error.</p>
<p>$$
\begin{align*}
MSE_\theta (T) ~ &amp;= ~ E_\theta \big{(} (T - \theta)^2 \big{)} \\
&amp;= ~ E_\theta \big{(} (D_\theta(T) + B_\theta(T))^2 \big{)} \\
&amp;= ~ E_\theta \big{(} D_\theta^2(T) + 2D_\theta(T)B_\theta(T) + B_\theta^2(T) \big{)} \\
&amp;= ~ E_\theta(D_\theta^2(T)) + 2B_\theta(T)E_\theta(D_\theta(T)) + B_\theta^2(T) \\
&amp;= ~ Var_\theta(T) + B_\theta^2(T)
\end{align*}
$$</p>
<p>The last line follows from two facts about the deviation $D_\theta(T)$:</p>
<ul>
<li>Deviations average out to zero: $E_\theta(D_\theta(T)) = 0$.</li>
<li>By the definition of variance, $Var_\theta(T) = E_\theta(D_\theta^2(T))$</li>
</ul>
<p>The <em>bias-variance decomposition</em> says</p>
<p>$$
\text{mean squared error} ~ = ~ \text{variance} + \text{bias}^2
$$</p>
<p>This quantifies what we saw visually: the quality of an estimator depends on the bias as well as the variance.</p>
<p>Ideally, we would like to construct an estimator for which both the bias and the variance are small. Sometimes this turns out to be impossible. For example, if you adjust an estimator to reduce bias, you might end up increasing the variance.</p>

</div>
</div>
</div>
</div>

 

