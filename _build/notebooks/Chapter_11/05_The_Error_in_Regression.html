---
redirect_from:
  - "/notebooks/chapter-11/05-the-error-in-regression"
interact_link: content/notebooks/Chapter_11/05_The_Error_in_Regression.ipynb
kernel_name: python3
has_widgets: false
title: |-
  The Error in Regression
prev_page:
  url: /notebooks/Chapter_11/04_Bounds_on_Correlation.html
  title: |-
    Bounds on Correlation
next_page:
  url: /notebooks/Chapter_11/06_Exercises.html
  title: |-
    Exercises
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Error-in-Regression">The Error in Regression<a class="anchor-link" href="#The-Error-in-Regression"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To assess the accuracy of the regression estimate, we must quantify the amount of error in the estimate. The error in the regression estimate is called the <em>residual</em> and is defined as</p>
<p>$$
D = Y - \hat{Y}
$$</p>
<p>where $\hat{Y} = \hat{a}(X-\mu_X) + \mu_Y$ is the regression estimate of $Y$ based on $X$.</p>
<p>Calculations become much easier if we express the residual $D$ in terms of the deviations $D_X$ and $D_Y$.</p>
<p>$$
\begin{align*} 
D ~ &amp;= ~ Y - \big{(} \hat{a}(X - \mu_X) + \mu_Y \big{)} \\
&amp;= ~ (Y - \mu_Y) - \hat{a}(X - \mu_X) \\
&amp;= ~ D_Y - \hat{a}D_X
\end{align*}
$$</p>
<p>Since $E(D_X) = 0 = E(D_Y)$, we have $E(D) = 0$.</p>
<p>This is consistent with what you learned in <a href="https://www.inferentialthinking.com/chapters/15/6/Numerical_Diagnostics.html#Average-of-Residuals">Data 8</a>: No matter what the shape of the scatter diagram, the average of the residuals is $0$.</p>
<p>In our probability world, "no matter what the shape of the scatter diagram" translates to "no matter what the joint distribution of $X$ and $Y$". Remember that we have made no assumptions about that joint distribution.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Mean-Squared-Error-of-Regression">Mean Squared Error of Regression<a class="anchor-link" href="#Mean-Squared-Error-of-Regression"> </a></h3><p>The mean squared error of regression is $E\big{(} (Y - \hat{Y})^2 \big{)}$. That is just $E(D^2)$, the expected squared residual.</p>
<p>Since $E(D) = 0$, $E(D^2) = Var(D)$. So the mean squared error of regression is the variance of the residual.</p>
<p>Let $r(X,Y) = r$ for short. To calculate the mean squared error of regression, recall that $\hat{a} = r\frac{\sigma_Y}{\sigma_X}$ and $E(D_XD_Y) = r\sigma_X\sigma_Y$.</p>
<p>The mean squared error of regression is</p>
<p>$$
\begin{align*}
Var(D) ~ &amp;= ~ E(D^2) \\
&amp;= ~ E(D_Y^2) - 2\hat{a}E(D_XD_Y) + \hat{a}^2E(D_X^2) \\
&amp;= ~ \sigma_Y^2 - 2r\frac{\sigma_Y}{\sigma_X}r\sigma_X\sigma_Y + r^2\frac{\sigma_Y^2}{\sigma_X^2}\sigma_X^2 \\
&amp;= ~ \sigma_Y^2 - 2r^2\sigma_Y^2 + r^2\sigma_Y^2 \\
&amp;= ~ \sigma_Y^2 - r^2\sigma_Y^2 \\
&amp;= ~ (1-r^2)\sigma_Y^2
\end{align*}
$$</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="SD-of-the-Residual">SD of the Residual<a class="anchor-link" href="#SD-of-the-Residual"> </a></h3><p>The SD of the residual is therefore</p>
<p>$$
SD(D) ~ = ~ \sqrt{1 - r^2}\sigma_Y
$$</p>
<p>which is consistent with the <a href="https://www.inferentialthinking.com/chapters/15/6/Numerical_Diagnostics.html#SD-of-the-Residuals">Data 8 formula</a>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$r$-As-a-Measure-of-Linear-Association">$r$ As a Measure of Linear Association<a class="anchor-link" href="#$r$-As-a-Measure-of-Linear-Association"> </a></h3><p>The expectation of the residual is always $0$. So if $SD(D) \approx 0$ then $D$ is pretty close to $0$ with high probability, that is, $Y$ is pretty close to $\hat{Y}$. In other words, if the SD of the residual is small, then $Y$ is pretty close to being a linear function of $X$.</p>
<p>The SD of the residual is small if $r$ is close to $1$ or $-1$. The closer $r$ is to those extremes, the closer $Y$ is to being a linear function of $X$. If $r = \pm 1$ then $Y$ is a perfectly linear function of $X$.</p>
<p>A way to visualize this is that if $r$ is close to $1$ or $-1$, and you repeatedly simulate points $(X, Y)$, the points will lie very close to a straight line. In that sense $r$ is a measure of how closely the scatter diagram is clustered around a straight line.</p>
<p>The case $r=0$ is worth examining. In that case we say that $X$ and $Y$ are "uncorrelated". Because $\hat{a} = 0$, the equation of the regression line is simply $\hat{Y} = \mu_Y$. That's the horizontal line at $\mu_Y$; your prediction for $Y$ is $\mu_Y$ no matter what the value of $X$ is. The mean squared error is therefore $E\big{(}(Y-\mu_Y)^2\big{)} = \sigma_Y^2$, which is exactly what you get by plugging $r=0$ into the expression $(1 - r^2)\sigma_Y^2$.</p>
<p>This shows that when $X$ and $Y$ are uncorrelated there is no benefit in using linear regression to estimate $Y$ based on $X$. In this sense too, $r$ quantifies the amount of linear association between $X$ and $Y$.</p>
<p>In exercises you will see that it is possible for $X$ and $Y$ to be uncorrelated and have a very strong <em>non-linear</em> association. So it is important to keep in mind that $r$ measures only linear association.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Residual-is-Uncorrelated-with-$X$">The Residual is Uncorrelated with $X$<a class="anchor-link" href="#The-Residual-is-Uncorrelated-with-$X$"> </a></h3><p>In Data 8 you learned to perform some <a href="https://www.inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html#Visual-Diagnostics">visual diagnostics</a> on regression by drawing a <em>residual plot</em> which was defined as a scatter diagram of the residuals and the observed values of $X$. We said that residual plots are always flat: "the plot shows no upward or downward trend."</p>
<p>We will now make this precise by showing that the correlation between $X$ and the residual $D$ is 0.</p>
<p>By the definition of correlation,</p>
<p>$$
\begin{align*}
r(D, X) ~ &amp;= ~ E\Big{(} \big{(}\frac{D - \mu_D}{\sigma_D}\big{)}\big{(}\frac{X - \mu_X}{\sigma_X}\big{)} \Big{)} \\ \\
&amp;= ~ \frac{1}{\sigma_D\sigma_X}E(DD_X)
\end{align*}
$$</p>
<p>because $\mu_D = 0$. Therefore to show $r(D, X) = 0$, we just have to show that $E(DD_X) = 0$. Let's do that.</p>
<p>$$
\begin{align*}
E(DD_X) ~ &amp;= ~ E((D_Y - \hat{a}D_X)D_X) \\
&amp;= ~ E(D_XD_Y) - \hat{a}E(D_X^2) \\
&amp;= ~ r\sigma_X\sigma_Y - r\frac{\sigma_Y}{\sigma_X}\sigma_X^2 \\
&amp;= ~ 0
\end{align*}
$$</p>

</div>
</div>
</div>
</div>

 

