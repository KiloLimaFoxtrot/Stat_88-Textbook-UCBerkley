---
redirect_from:
  - "/notebooks/chapter-11/06-exercises"
interact_link: content/notebooks/Chapter_11/06_Exercises.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Exercises
prev_page:
  url: /notebooks/Chapter_11/05_The_Error_in_Regression.html
  title: |-
    The Error in Regression
next_page:
  url: 
  title: |-
    
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exercises">Exercises<a class="anchor-link" href="#Exercises"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>1.</strong> 
A newspaper article called <a href="https://www.theguardian.com/world/2006/jul/20/secondworldwar.tvandradio">How a Statistical Formula Won the War</a> reports a formula for estimating the number of German tanks under the model we have assumed n Section 11.2.</p>
<p>Compare the formula with the "augmented maximum" estimator $T_3$ of Section 11.2 (which is what was actually used in the war). Are the two the same? If not, what is the bias of the estimator in the report? How does the variance of that estimator compare with the variance of the augmented maximum $T_3$?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>2.</strong> 
Let $\theta_1 &lt; \theta_2$ and suppose $X_1, X_2, \ldots, X_n$ are i.i.d. uniform on the interval $(\theta_1, \theta_2)$. Let $\theta = \theta_2 - \theta_1$ be the length of the interval.</p>
<p><strong>a)</strong> Let $M_1 = \min(X_1, X_2, \ldots, X_n)$ be the sample minimum and $M_2 = \max(X_1, X_2, \ldots, X_n)$ the sample max. The statistic $T_1 = M_2 - M_1$ is called the <em>range</em> of the sample and is a natural estimator of $\theta$. Without calculation, explain why $T_1$ is biased, and say whether it underestimates or overestimates $\theta$.</p>
<p><strong>b)</strong> Find the bias of $T_1$ and confirm that its sign is consistent with your answer to Part <strong>a</strong>. For large $n$, is the size of the bias large or small?</p>
<p><strong>c)</strong> Use $T_1$ to construct an unbiased estimator of $\theta$. Call the new estimator $T_2$.</p>
<p><strong>d)</strong> Compare $SD(T_1)$ and $SD(T_2)$. Which one is bigger? For large $n$, is it a lot bigger or just a bit bigger?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>3.</strong>
Sometimes data scientists want to fit a linear model that has no intercept term. For example, this might be the case when the data are from a scientific experiement in which the attribute $X$ can have values near $0$ and there is a physical reason why the response $Y$ must be $0$ when $X=0$.</p>
<p>So let $(X, Y)$ be a random pair and suppose you want to predict $Y$ by an estimator of the form $aX$ for some $a$. Find the least squares predictor $\hat{Y}$ among all predictors of this form.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>4.</strong>
Find the mean squared error of the least squares predictor in the previous exercise, and hence prove the <em>Cauchy-Schwarz Inequality</em>:</p>
<p>$$
\vert E(XY) \vert ~ \le ~ \sqrt{E(X^2)E(Y^2)}
$$</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>5.</strong>
Let $(X, Y)$ be a random pair.</p>
<p><strong>a)</strong> For constants $a \ne 0$ and $b$, let $V = aX + b$. Apply the definition of correlation to find $r(X, V)$. [It might be easier to separate the cases $a &lt; 0$ and $a &gt; 0$.]</p>
<p><strong>b)</strong> For constants $a \ne 0$ and $b$, let $W = aY + b$. Find $r(X, W)$ in terms of $r(X, Y)$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>6</strong>.
Let $(X, Y)$ be a random pair and let $\hat{Y}$ be the least squares linear predcitor of $Y$ based on $X$. Assume $r(X, Y) \ne 0$.</p>
<p><strong>a)</strong> Find $r(X, \hat{Y})$.</p>
<p><strong>b)</strong> Let $D = Y - \hat{Y}$ be the residual. Find $r(D, \hat{Y})$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>7.</strong>
Let $(X, Y)$ be a random pair and let $r = r(X,Y)$. Let $X^*$ be $X$ in standard units and let $Y^*$ be $Y$ in standard units.</p>
<p><strong>a)</strong> Find $r(X^*, Y^*)$.</p>
<p><strong>b)</strong> Write the equation for $\hat{Y^*}$, the least squares linear predictor of $Y^*$ based on $X^*$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>8.</strong> 
It can be shown that for many football-shaped scatter diagrams it is OK to assume that each of the two variables is normally distributed.</p>
<p>Suppose that a large number of students take two tests (like the Math and Verbal SAT), and suppose that the scatter diagram of the two scores is football shaped with a correlation of 0.6.</p>
<p><strong>a)</strong> Let $(X, Y)$ be the scores of a randomly picked student, and suppose $X$ is on the the 90th percentile. Estimate the percentile rank of $Y$.</p>
<p><strong>b)</strong> Let $(X, Y)$ be the score of a randomly picked student, and suppose $Y$ is on the 78th percentile. Estimate the percentile rank of $X$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>9.</strong>
Let $(X, Y)$ be a random pair and let $\hat{Y}$ be the linear regression estimate of $Y$ based on $X$. Let $D = Y - \hat{Y}$ be the residual.</p>
<p>Justify the <em>decomposition of varinance</em> formula $Var(Y) = Var(\hat{Y}) + Var(D)$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>10.</strong>
Let $X$ be a random variable with expectation $\mu_X$ and SD $\sigma_X$. Suppose you are going to use a constant $c$ as your predictor of $X$.</p>
<p><strong>a)</strong> Let $MSE(c)$ be the mean squared error of the predictor $c$. Write a formula for $MSE(c)$.</p>
<p><strong>b)</strong> Find $\hat{c}$, the least squares constant predictor.</p>
<p><strong>c)</strong> Find $MSE(\hat{c})$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>11.</strong>
Let $X$ have the uniform distribution on the three points $-1$, $0$, and $1$. Let $Y = X^2$.</p>
<p><strong>a)</strong> Show that $X$ and $Y$ are uncorrelated.</p>
<p><strong>b)</strong> Are $X$ and $Y$ independent?</p>

</div>
</div>
</div>
</div>

 

