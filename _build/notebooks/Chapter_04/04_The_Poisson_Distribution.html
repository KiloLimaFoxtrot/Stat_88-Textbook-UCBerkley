---
redirect_from:
  - "/notebooks/chapter-04/04-the-poisson-distribution"
interact_link: content/notebooks/Chapter_04/04_The_Poisson_Distribution.ipynb
kernel_name: python3
has_widgets: false
title: |-
  The Poisson Distribution
prev_page:
  url: /notebooks/Chapter_04/03_Exponential_Approximations.html
  title: |-
    Exponential Approximations
next_page:
  url: 
  title: |-
    
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Poisson-Distribution">The Poisson Distribution<a class="anchor-link" href="#The-Poisson-Distribution"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Data scientists use probability distributions as <em>models</em> for how their data are generated. In this context a model is a set of assumptions involving probabilities. Almost invariably, models are simplified representations of complex real scenarios.</p>
<p>The <em>Poisson</em> distribution is sometimes used to model the number of times a rare event occurs. It is named after its originator, the French mathematician, scientist, and engineer <a href="https://en.wikipedia.org/wiki/Sim%C3%A9on_Denis_Poisson">Sim√©on Denis Poisson</a>. The parameter of the Poisson distribution is a positive number which we will call $\mu$. A random variable $X$ has the Poisson distribution with parameter $\mu$ if</p>
<p>$$
P(X = k) ~ = ~ e^{-\mu} \frac{\mu^k}{k!}, ~~~ k = 0, 1, 2, \ldots
$$</p>
<p>The possible values of $X$ are all the non-negative integers. That's an infinite set.</p>
<p>The formula does indeed define a probability distribution:</p>
<p>$$
\sum_{k=0}^\infty e^{-\mu} \frac{\mu^k}{k!} ~ = ~ e^{-\mu} \sum_{k=0}^\infty \frac{\mu^k}{k!} ~ = ~ e^{-\mu} \cdot e^\mu ~ = ~ 1
$$</p>
<p>Notice that the probabilities in the Poisson distribution are proportional to the terms in the expansion of $e^\mu$. The multiplicative constant $e^{-\mu}$ makes the probabilities add up to 1.</p>
<p>The figure below is the probability histogram of the Poisson distribution in the case $\mu = 1.5$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_04/04_The_Poisson_Distribution_3_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Even though there are infinitely many possible values, the probable values are a pretty small set, between 0 and about 6 or 7.</p>
<p>The tallest bar is at $k = 1$. This value 1 is called the <em>mode</em> of the distribution. It can be shown that the mode of the Poisson distribution is the integer part of $\mu$. If $\mu$ is an integer then there are two modes, $\mu$ and $\mu - 1$.</p>
<p>Here is the histogram of the Poisson $(3)$ distribution. There are two modes, at 3 and 2.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_04/04_The_Poisson_Distribution_5_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The histograms show why you might want to use these distributions as models for random counts that tend to be small.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Bombing-of-London">The Bombing of London<a class="anchor-link" href="#The-Bombing-of-London"> </a></h3><p>During World War II the German air force launched a massive bombing attack on Britain and on London in particular. The British press called this attack the Blitz, the German word for lightning.</p>
<p>A classic probability <a href="https://archive.org/details/AnIntroductionToProbabilityTheoryAndItsApplicationsVolume1">text by William Feller</a> provides data about the number of times bombs fell in different parts of London. The data were recorded as follows.</p>
<p>The city was divided into 576 sub-regions which we will call locations. Officials recorded the number of times each location was hit by a bomb. The distribution is shown in the histogram below. It shows that about 40% of the locations did not suffer a hit. But about 60% did, and some were hit multiple times. A small percent of locations were hit five or more times. Those are not visible on the scale of this graph.</p>
<p><img src="../../images/blitz_dist.png" alt="Number of bomb hits"></p>
<p>Feller's book shows how this distribution is well approximated by the Poisson distribution with parameter $\mu = 0.93$. In the next chapter, we will see how the parameter was calculated. For now, see how closely the Poisson (0.93) distribution resembles the distribution of the data.</p>
<p><img src="../../images/blitz_poisson.png" alt="Blitz Poisson approximation"></p>
<p>The numerical values of the Poisson probabilities have been calculated using a call that should come as no surprise.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># stats.poisson.pmf(k_array, mu)</span>

<span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">),</span> <span class="mf">0.93</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([0.39455371, 0.36693495, 0.17062475, 0.05289367, 0.01229778,
       0.00228739])</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Defective-Drives">Defective Drives<a class="anchor-link" href="#Defective-Drives"> </a></h3><p>You can work with Poisson distributions just as you worked with all the other distributions that we have studied. Here is an example.</p>
<p>A manufacturing process produces large cases of USB flash drives. In each case, the number of defective drives has the Poisson (2.5) distribution, independent of all other cases.</p>
<p>What is the chance that all of the next five cases contain more than one defective drive?</p>
<p>$$
\begin{align*}
&amp; P(\text{Case 1 contains at more than one defective drive}) \\
&amp; = ~ 1 - \big{(} e^{-2.5}\frac{2.5^0}{0!} + e^{-2.5}\frac{2.5^1}{1!} \big{)} \\
&amp; = ~ p ~ \approx ~ 71.27\%
\end{align*}
$$</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">p</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.7127025048163542</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The answer is $p^5$ which is about 18.39%.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">**</span> <span class="mi">5</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.18388293444804887</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sums-of-Independent-Poisson-Random-Variables">Sums of Independent Poisson Random Variables<a class="anchor-link" href="#Sums-of-Independent-Poisson-Random-Variables"> </a></h3><p>A useful property of the Poisson distribution is that if $X$ and $Y$ are random variables such that</p>
<ul>
<li>$X$ and $Y$ are independent,</li>
<li>$X$ has the Poisson $(\mu)$ distribution, and</li>
<li>$Y$ has the Poisson $(\lambda)$ distribution,</li>
</ul>
<p>then the sum $S = X + Y$ has the Poisson $(\mu + \lambda)$ distribution.</p>
<p>In this course you don't have to know how to prove this though the proof is not very hard. But you do have to be able to apply it, as in the following example.</p>
<p>An office building has three parking lots. For $i = 1, 2, 3$ let $X_i$ be the number of illegaly parked cars in Lot $i$, and let $X_i$ have the Poisson distribution with parameter $i$. Assume that $X_1, X_2, X_3$ are independent of each other.</p>
<p>What is the chance that there are no more than 10 illegally parked cars in all three lots combined?</p>
<p>The toal number of illegally parked cars $S = X_1 + X_2 + X_3$ has the Poisson distribution with parameter $1+2+3 = 6$.</p>
<p>$$
P(S \le 10) ~ = ~ \sum_{k=0}^{10} e^{-6} \frac{6^k}{k!} ~ \approx ~ 95.74\%
$$</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.957379076417462</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Law-of-Small-Numbers">The Law of Small Numbers<a class="anchor-link" href="#The-Law-of-Small-Numbers"> </a></h3><p>You may have noticed that the histograms of the Poisson distributions in this section don't look very different from histograms of the binomial distribution when the success probability $p$ is small. Indeed, one way in which the Poisson distribution arises in practice is as an approximation to a binomial distribution that has a large $n$ and a small $p$.</p>
<p>Let $X$ have the binomial $(n, p)$ distribution where $n$ is large and $p$ is small. Let $\mu = np$. We saw in the previous section that</p>
<p>$$
P(X = 0) ~ = ~ (1-p)^n ~ \approx ~ e^{-\mu} ~ = ~ e^{-\mu} \frac{\mu^0}{0!}
$$</p>
<p>which is the Poisson $(\mu)$ probability of 0 successes.</p>
<p>Next,</p>
<p>$$
\begin{align*}
P(X = 1) ~ = ~ \binom{n}{1}p(1-p)^{n-1} ~ &amp;= ~ np\frac{(1-p)^n}{1-p} \\
&amp;\approx ~ \mu e^{-\mu} ~ = ~ e^{-\mu} \frac{\mu^1}{1!}
\end{align*}
$$</p>
<p>which is the Poisson $(\mu)$ probability of 1 success.</p>
<p>By continuing in this way, with some care in the algebra and the limits, it can be shown that each binomial term is approximated by the corresponding Poisson term.</p>
<p>Thus <strong>when $n$ is large and $p$ is small, the binomial $(n, p)$ distribution is well approximated by the Poisson $(\mu)$ distribution where $\mu = np$.</strong></p>
<p>This is called the <em>law of small numbers</em> and is one of the main reasons the Poisson distribution is a good model for counting rare events.</p>
<p>The graph below shows overlaid histograms of the binomial $(100, 1/100)$ and Poisson $(1)$ distributions. Does it look as though you can see only one histogram? That's because the Poisson approximation is excellent!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_04/04_The_Poisson_Distribution_16_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Suppose you run $n$ independent, identically distributed success/failure trials with chance $1/n$ of success on any given trial. Let $X$ be the number of successes.</p>
<p>Then $X$ has the binomial $(n, 1/n)$ distribution which is well approximated by the Poisson $(1)$ distribution when $n$ is large. If you increase $n$ still further, the distribution of $X$ won't change much. It will still look like the Poisson $(1)$ distribution.</p>

</div>
</div>
</div>
</div>

 

