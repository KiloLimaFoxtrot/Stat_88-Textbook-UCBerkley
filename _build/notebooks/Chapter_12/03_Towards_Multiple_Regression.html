---
redirect_from:
  - "/notebooks/chapter-12/03-towards-multiple-regression"
interact_link: content/notebooks/Chapter_12/03_Towards_Multiple_Regression.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Towards Multiple Regression
prev_page:
  url: /notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope.html
  title: |-
    The Distribution of the Estimated Slope
next_page:
  url: /notebooks/Chapter_12/04_Exercises.html
  title: |-
    Exercsies
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Towards-Multiple-Regression">Towards Multiple Regression<a class="anchor-link" href="#Towards-Multiple-Regression"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This section is an extended example of applications of the methods we have derived for regression. We will start with simple regression, which we understand well, and will then indicate how some of the results can be extended when there is more than one predictor variable.</p>
<p>The data are from a study on the treatment of Hodgkin's disease, a type of cancer that can affect young people. The good news is that treatments for this cancer have <a href="https://en.wikipedia.org/wiki/Hodgkin_lymphoma#Prognosis">high success rates</a>. The bad news is that the treatments can be rather strong combinations of chemotherapy and radiation, and thus have serious side effects. A goal of the study was to identify combinations of treatments with reduced side effects.</p>
<p>The table <code>hodgkins</code> contains data on a random sample of patients. Each row corresponds to a patient. The columns contain the patient's height in centimeters, the amount of radiation, the amount of medication used in chemotherapy, and measurements on the health of the patient's lungs.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hodgkins</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>height</th> <th>rad</th> <th>chemo</th> <th>base</th> <th>month15</th> <th>difference</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>164   </td> <td>679 </td> <td>180  </td> <td>160.57</td> <td>87.77  </td> <td>-72.8     </td>
        </tr>
        <tr>
            <td>168   </td> <td>311 </td> <td>180  </td> <td>98.24 </td> <td>67.62  </td> <td>-30.62    </td>
        </tr>
        <tr>
            <td>173   </td> <td>388 </td> <td>239  </td> <td>129.04</td> <td>133.33 </td> <td>4.29      </td>
        </tr>
        <tr>
            <td>157   </td> <td>370 </td> <td>168  </td> <td>85.41 </td> <td>81.28  </td> <td>-4.13     </td>
        </tr>
        <tr>
            <td>160   </td> <td>468 </td> <td>151  </td> <td>67.94 </td> <td>79.26  </td> <td>11.32     </td>
        </tr>
        <tr>
            <td>170   </td> <td>341 </td> <td>96   </td> <td>150.51</td> <td>80.97  </td> <td>-69.54    </td>
        </tr>
        <tr>
            <td>163   </td> <td>453 </td> <td>134  </td> <td>129.88</td> <td>69.24  </td> <td>-60.64    </td>
        </tr>
        <tr>
            <td>175   </td> <td>529 </td> <td>264  </td> <td>87.45 </td> <td>56.48  </td> <td>-30.97    </td>
        </tr>
        <tr>
            <td>185   </td> <td>392 </td> <td>240  </td> <td>149.84</td> <td>106.99 </td> <td>-42.85    </td>
        </tr>
        <tr>
            <td>178   </td> <td>479 </td> <td>216  </td> <td>92.24 </td> <td>73.43  </td> <td>-18.81    </td>
        </tr>
    </tbody>
</table>
<p>... (12 rows omitted)</p>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="n">hodgkins</span><span class="o">.</span><span class="n">num_rows</span>
<span class="n">n</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>22</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The radiation was directed towards each patient's chest area or "mantle", to destroy cancer cells in the lymph nodes near that area. Since this could adversely affect the patients' lungs, the researchers measured the health of the patients' lungs both before and after treatment. Each patient received a score, with larger scores corresponding to more healthy lungs.</p>
<p>The table records the baseline scores and also the scores 15 months after treatment. The change in score (15 month score minus baseline score) is in the final column. Notice the negative differences: 15 months after treatment, many patients' lungs weren't doing as well as before the treatment.</p>
<p>Perhaps not surprisingly, patients with larger baseline scores had bigger drops in score.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hodgkins</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;base&#39;</span><span class="p">,</span> <span class="s1">&#39;difference&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_12/03_Towards_Multiple_Regression_7_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will regress the difference on the baseline score, this time using the Python module <code>statsmodels</code> that allows us to easily perform multiple regression as well. You don't have to learn the code below (though it's not hard). Just focus on understanding an interpreting the output.</p>
<p>As a first step, we must import the module.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>Table</code> method <code>to_df</code> allows us to convert the table <code>hodgkins</code> to a structure called a data frame that works more smoothly with <code>statsmodels</code>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h_data</span> <span class="o">=</span> <span class="n">hodgkins</span><span class="o">.</span><span class="n">to_df</span><span class="p">()</span>
<span class="n">h_data</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>height</th>
      <th>rad</th>
      <th>chemo</th>
      <th>base</th>
      <th>month15</th>
      <th>difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>164</td>
      <td>679</td>
      <td>180</td>
      <td>160.57</td>
      <td>87.77</td>
      <td>-72.80</td>
    </tr>
    <tr>
      <td>1</td>
      <td>168</td>
      <td>311</td>
      <td>180</td>
      <td>98.24</td>
      <td>67.62</td>
      <td>-30.62</td>
    </tr>
    <tr>
      <td>2</td>
      <td>173</td>
      <td>388</td>
      <td>239</td>
      <td>129.04</td>
      <td>133.33</td>
      <td>4.29</td>
    </tr>
    <tr>
      <td>3</td>
      <td>157</td>
      <td>370</td>
      <td>168</td>
      <td>85.41</td>
      <td>81.28</td>
      <td>-4.13</td>
    </tr>
    <tr>
      <td>4</td>
      <td>160</td>
      <td>468</td>
      <td>151</td>
      <td>67.94</td>
      <td>79.26</td>
      <td>11.32</td>
    </tr>
    <tr>
      <td>5</td>
      <td>170</td>
      <td>341</td>
      <td>96</td>
      <td>150.51</td>
      <td>80.97</td>
      <td>-69.54</td>
    </tr>
    <tr>
      <td>6</td>
      <td>163</td>
      <td>453</td>
      <td>134</td>
      <td>129.88</td>
      <td>69.24</td>
      <td>-60.64</td>
    </tr>
    <tr>
      <td>7</td>
      <td>175</td>
      <td>529</td>
      <td>264</td>
      <td>87.45</td>
      <td>56.48</td>
      <td>-30.97</td>
    </tr>
    <tr>
      <td>8</td>
      <td>185</td>
      <td>392</td>
      <td>240</td>
      <td>149.84</td>
      <td>106.99</td>
      <td>-42.85</td>
    </tr>
    <tr>
      <td>9</td>
      <td>178</td>
      <td>479</td>
      <td>216</td>
      <td>92.24</td>
      <td>73.43</td>
      <td>-18.81</td>
    </tr>
    <tr>
      <td>10</td>
      <td>179</td>
      <td>376</td>
      <td>160</td>
      <td>117.43</td>
      <td>101.61</td>
      <td>-15.82</td>
    </tr>
    <tr>
      <td>11</td>
      <td>181</td>
      <td>539</td>
      <td>196</td>
      <td>129.75</td>
      <td>90.78</td>
      <td>-38.97</td>
    </tr>
    <tr>
      <td>12</td>
      <td>173</td>
      <td>217</td>
      <td>204</td>
      <td>97.59</td>
      <td>76.38</td>
      <td>-21.21</td>
    </tr>
    <tr>
      <td>13</td>
      <td>166</td>
      <td>456</td>
      <td>192</td>
      <td>81.29</td>
      <td>67.66</td>
      <td>-13.63</td>
    </tr>
    <tr>
      <td>14</td>
      <td>170</td>
      <td>252</td>
      <td>150</td>
      <td>98.29</td>
      <td>55.51</td>
      <td>-42.78</td>
    </tr>
    <tr>
      <td>15</td>
      <td>165</td>
      <td>622</td>
      <td>162</td>
      <td>118.98</td>
      <td>90.92</td>
      <td>-28.06</td>
    </tr>
    <tr>
      <td>16</td>
      <td>173</td>
      <td>305</td>
      <td>213</td>
      <td>103.17</td>
      <td>79.74</td>
      <td>-23.43</td>
    </tr>
    <tr>
      <td>17</td>
      <td>174</td>
      <td>566</td>
      <td>198</td>
      <td>94.97</td>
      <td>93.08</td>
      <td>-1.89</td>
    </tr>
    <tr>
      <td>18</td>
      <td>173</td>
      <td>322</td>
      <td>119</td>
      <td>85.00</td>
      <td>41.96</td>
      <td>-43.04</td>
    </tr>
    <tr>
      <td>19</td>
      <td>173</td>
      <td>270</td>
      <td>160</td>
      <td>115.02</td>
      <td>81.12</td>
      <td>-33.90</td>
    </tr>
    <tr>
      <td>20</td>
      <td>183</td>
      <td>259</td>
      <td>241</td>
      <td>125.02</td>
      <td>97.18</td>
      <td>-27.84</td>
    </tr>
    <tr>
      <td>21</td>
      <td>188</td>
      <td>238</td>
      <td>252</td>
      <td>137.43</td>
      <td>113.20</td>
      <td>-24.23</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are several variables we could use to predict the difference. The only one we wouldn't use is the 15 month measurement, as that's precisely what we won't have for a new patient before the treatment is adminstered.</p>
<p>But which of the rest should we use? One way to choose is to look at the <em>correlation matrix</em> of all the variables.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h_data</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>height</th>
      <th>rad</th>
      <th>chemo</th>
      <th>base</th>
      <th>month15</th>
      <th>difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>height</td>
      <td>1.000000</td>
      <td>-0.305206</td>
      <td>0.576825</td>
      <td>0.354229</td>
      <td>0.390527</td>
      <td>-0.043394</td>
    </tr>
    <tr>
      <td>rad</td>
      <td>-0.305206</td>
      <td>1.000000</td>
      <td>-0.003739</td>
      <td>0.096432</td>
      <td>0.040616</td>
      <td>-0.073453</td>
    </tr>
    <tr>
      <td>chemo</td>
      <td>0.576825</td>
      <td>-0.003739</td>
      <td>1.000000</td>
      <td>0.062187</td>
      <td>0.445788</td>
      <td>0.346310</td>
    </tr>
    <tr>
      <td>base</td>
      <td>0.354229</td>
      <td>0.096432</td>
      <td>0.062187</td>
      <td>1.000000</td>
      <td>0.561371</td>
      <td>-0.630183</td>
    </tr>
    <tr>
      <td>month15</td>
      <td>0.390527</td>
      <td>0.040616</td>
      <td>0.445788</td>
      <td>0.561371</td>
      <td>1.000000</td>
      <td>0.288794</td>
    </tr>
    <tr>
      <td>difference</td>
      <td>-0.043394</td>
      <td>-0.073453</td>
      <td>0.346310</td>
      <td>-0.630183</td>
      <td>0.288794</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each entry in this table is the correlation between the variable specified by the row label and the variable specified by the column label. That's why all the diagonal entries are $1$.</p>
<p>Look at the last column (or last row). This contains the correlation between <code>difference</code> and each of the other variables. The baseline measurement has the largest correlation. To run the regression of <code>difference</code> on <code>base</code> we must first extract the columns of data that we need and then use the appropriate <code>statsmodels</code> methods.</p>
<p>First, we create data frames corresponding to the response and the predictor variable. The methods are not the same as for <code>Tables</code>, but you will get a general sense of what they are doing.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">h_data</span><span class="p">[[</span><span class="s1">&#39;difference&#39;</span><span class="p">]]</span>  <span class="c1"># response</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">h_data</span><span class="p">[[</span><span class="s1">&#39;base&#39;</span><span class="p">]]</span>        <span class="c1"># predictor</span>

<span class="c1"># specify that the model includes an intercept</span>
<span class="n">x_with_int</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The name of the <code>OLS</code> method stands for Ordinary Least Squares, which is the kind of least squares that we have discussed. There are other more complicated kinds that you might encounter in more advanced classes.</p>
<p>There is a lot of output, some of which we will discuss and the rest of which we will leave to another class. For some reason the output includes the date and time of running the regression, right in the middle of the summary statistics.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">simple_regression</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x_with_int</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">simple_regression</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>difference</td>    <th>  R-squared:         </th> <td>   0.397</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.367</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   13.17</td>
</tr>
<tr>
  <th>Date:</th>             <td>Fri, 06 Dec 2019</td> <th>  Prob (F-statistic):</th>  <td>0.00167</td>
</tr>
<tr>
  <th>Time:</th>                 <td>22:34:07</td>     <th>  Log-Likelihood:    </th> <td> -92.947</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    22</td>      <th>  AIC:               </th> <td>   189.9</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>   192.1</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   32.1721</td> <td>   17.151</td> <td>    1.876</td> <td> 0.075</td> <td>   -3.604</td> <td>   67.949</td>
</tr>
<tr>
  <th>base</th>  <td>   -0.5447</td> <td>    0.150</td> <td>   -3.630</td> <td> 0.002</td> <td>   -0.858</td> <td>   -0.232</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.133</td> <th>  Durbin-Watson:     </th> <td>   1.774</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.568</td> <th>  Jarque-Bera (JB):  </th> <td>   0.484</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.362</td> <th>  Prob(JB):          </th> <td>   0.785</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.069</td> <th>  Cond. No.          </th> <td>    530.</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are three blocks of output. We will focus only on the the middle block.</p>
<ul>
<li><code>const</code> and <code>base</code> refer to the intercept and baseline measurement.</li>
<li><code>coef</code> stands for the estimated coefficients, which in our notation are $\hat{\beta_0}$ and $\hat{\beta_1}$.</li>
<li><code>t</code> is the $t$-statistic for testing whether or not the coefficient is 0. Based on our model, its degrees of freedom are $n-2 = 20$; you'll find this under <code>Df Residuals</code> in the top block.</li>
<li><code>P &gt; |t|</code> is the total area in the two tails of the $t$ distribution with $n-2$ degrees of freedom.</li>
<li><code>[0.025 0.975]</code> are the ends of a 95% confidence interval for the parameter.</li>
</ul>
<p>For the test of whether or not the true slope of the baseline measurement is $0$, the observed test statistic is</p>
<p>$$
\frac{-0.5447 - 0}{0.150} ~ = ~ -3.63
$$</p>
<p>The area in one tail is the chance that the $t$ distribution with $20$ degrees of freedom is less than $-3.63$. That's the cdf of the distribution evaluated at $-3.63$:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">one_tail</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mf">3.63</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">one_tail</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.0008339581409629714</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our test is two-sided (large values of $\vert t \vert$ favor the alternative), so the $p$-value of the test is the total area of two tails, which is the displayed value $0.002$ after rounding.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">one_tail</span>
<span class="n">p</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.0016679162819259429</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To find a 95% confidence interval for the true slope, we have to replace $2$ in the expression $\hat{\beta}_1 \pm 2SE(\hat{\beta}_1)$ by the corresponding value from the $t$ distribution with 20 degrees of freedom. That's not very far from $2$:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t_95</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">t_95</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2.0859634472658364</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A 95% confidence interval for the true slope is given by $\hat{\beta}_1 \pm t_{95}SE(\hat{\beta}_1)$. The observed interval is therefore given by the calculation below, which results in the same values as in the output of <code>sm.OLS</code> above.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 95% confidence interval for the true slope</span>

<span class="o">-</span><span class="mf">0.5447</span> <span class="o">-</span> <span class="n">t_95</span><span class="o">*</span><span class="mf">0.150</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5447</span> <span class="o">+</span> <span class="n">t_95</span><span class="o">*</span><span class="mf">0.150</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(-0.8575945170898753, -0.23180548291012454)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multiple-Regression">Multiple Regression<a class="anchor-link" href="#Multiple-Regression"> </a></h3><p>What if we wanted to regress <code>difference</code> on both <code>base</code> and <code>chemo</code>? The first thing to do would be to check the correlation matrix again:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">h_data</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>height</th>
      <th>rad</th>
      <th>chemo</th>
      <th>base</th>
      <th>month15</th>
      <th>difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>height</td>
      <td>1.000000</td>
      <td>-0.305206</td>
      <td>0.576825</td>
      <td>0.354229</td>
      <td>0.390527</td>
      <td>-0.043394</td>
    </tr>
    <tr>
      <td>rad</td>
      <td>-0.305206</td>
      <td>1.000000</td>
      <td>-0.003739</td>
      <td>0.096432</td>
      <td>0.040616</td>
      <td>-0.073453</td>
    </tr>
    <tr>
      <td>chemo</td>
      <td>0.576825</td>
      <td>-0.003739</td>
      <td>1.000000</td>
      <td>0.062187</td>
      <td>0.445788</td>
      <td>0.346310</td>
    </tr>
    <tr>
      <td>base</td>
      <td>0.354229</td>
      <td>0.096432</td>
      <td>0.062187</td>
      <td>1.000000</td>
      <td>0.561371</td>
      <td>-0.630183</td>
    </tr>
    <tr>
      <td>month15</td>
      <td>0.390527</td>
      <td>0.040616</td>
      <td>0.445788</td>
      <td>0.561371</td>
      <td>1.000000</td>
      <td>0.288794</td>
    </tr>
    <tr>
      <td>difference</td>
      <td>-0.043394</td>
      <td>-0.073453</td>
      <td>0.346310</td>
      <td>-0.630183</td>
      <td>0.288794</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What you are looking for is not just that <code>chemo</code> is the next most highly correlated with <code>difference</code> after <code>base</code>. More importantly, you are looking to see how strongly the two predictor variables <code>base</code> and <code>chemo</code> are linearly related <em>to each other</em>. That is, you are trying to figure out whether the two variables pick up genuinely different dimensions of the data.</p>
<p>The correlation matrix shows that the correlation between <code>base</code> and <code>chemo</code> is only about $0.06$. The two predictors are not close to being linear functions of each other. So let's run the regression.</p>
<p>The code is exactly the same as before, except that we have included a second predictor variable.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">h_data</span><span class="p">[[</span><span class="s1">&#39;difference&#39;</span><span class="p">]]</span>      <span class="c1"># response</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">h_data</span><span class="p">[[</span><span class="s1">&#39;base&#39;</span><span class="p">,</span> <span class="s1">&#39;chemo&#39;</span><span class="p">]]</span>  <span class="c1"># predictors</span>

<span class="c1"># specify that the model includes an intercept</span>
<span class="n">x2_with_int</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span> 

<span class="n">multiple_regression</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x2_with_int</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">multiple_regression</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>difference</td>    <th>  R-squared:         </th> <td>   0.546</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.499</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   11.44</td>
</tr>
<tr>
  <th>Date:</th>             <td>Fri, 06 Dec 2019</td> <th>  Prob (F-statistic):</th> <td>0.000548</td>
</tr>
<tr>
  <th>Time:</th>                 <td>22:34:31</td>     <th>  Log-Likelihood:    </th> <td> -89.820</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    22</td>      <th>  AIC:               </th> <td>   185.6</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    19</td>      <th>  BIC:               </th> <td>   188.9</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   -0.9992</td> <td>   20.227</td> <td>   -0.049</td> <td> 0.961</td> <td>  -43.335</td> <td>   41.336</td>
</tr>
<tr>
  <th>base</th>  <td>   -0.5655</td> <td>    0.134</td> <td>   -4.226</td> <td> 0.000</td> <td>   -0.846</td> <td>   -0.285</td>
</tr>
<tr>
  <th>chemo</th> <td>    0.1898</td> <td>    0.076</td> <td>    2.500</td> <td> 0.022</td> <td>    0.031</td> <td>    0.349</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.853</td> <th>  Durbin-Watson:     </th> <td>   1.781</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.653</td> <th>  Jarque-Bera (JB):  </th> <td>   0.368</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.317</td> <th>  Prob(JB):          </th> <td>   0.832</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.987</td> <th>  Cond. No.          </th> <td>1.36e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.36e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Igore the scary warning above. There isn't strong multicollinearity (predictor variables being highly correlated with each other) nor other serious issues.</p>
<p>Just focus on the middle block of the output. It's just like the middle block of the simple regression output, with one more line corresponding to <code>chemo</code>.</p>
<p>All of the values in the block are interpreted in the same way as before. The only change is in the degrees of freedom: because you are estimating one more parameter, the degrees of freedom have dropped by $1$, and are thus $19$ instead of $20$.</p>
<p>For example, the 95% confidence interval for the slope of <code>chemo</code> is calculated as follows.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t_95_df19</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">,</span> <span class="mi">19</span><span class="p">)</span>

<span class="mf">0.1898</span> <span class="o">-</span> <span class="n">t_95_df19</span><span class="o">*</span><span class="mf">0.076</span><span class="p">,</span> <span class="mf">0.1898</span> <span class="o">+</span> <span class="n">t_95_df19</span><span class="o">*</span><span class="mf">0.076</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(0.03073017186497201, 0.348869828135028)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, take a look at the value of <code>R-squared</code> in the very top line. It is $0.546$ compared to $0.397$ for the simple regression. It's a math fact that the more predictor variables you use, the higher the <code>R-squared</code> value will be. This tempts people into using lots of predictors, whether or not the resulting model is comprehensible.</p>
<p>With an "everything as well as the kitchen sink" approach to selecting predictor variables, a researcher might be inclined to use all the possible predictors.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">h_data</span><span class="p">[[</span><span class="s1">&#39;difference&#39;</span><span class="p">]]</span>      <span class="c1"># response</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">h_data</span><span class="p">[[</span><span class="s1">&#39;base&#39;</span><span class="p">,</span> <span class="s1">&#39;chemo&#39;</span><span class="p">,</span> <span class="s1">&#39;rad&#39;</span><span class="p">,</span> <span class="s1">&#39;height&#39;</span><span class="p">]]</span>  <span class="c1"># predictors</span>

<span class="c1"># specify that the model includes an intercept</span>
<span class="n">x3_with_int</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span> 

<span class="n">bad_regression</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x3_with_int</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">bad_regression</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>difference</td>    <th>  R-squared:         </th> <td>   0.550</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.444</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.185</td>
</tr>
<tr>
  <th>Date:</th>             <td>Fri, 06 Dec 2019</td> <th>  Prob (F-statistic):</th>  <td>0.00645</td>
</tr>
<tr>
  <th>Time:</th>                 <td>22:50:02</td>     <th>  Log-Likelihood:    </th> <td> -89.741</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    22</td>      <th>  AIC:               </th> <td>   189.5</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    17</td>      <th>  BIC:               </th> <td>   194.9</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>  <td>   33.5226</td> <td>  101.061</td> <td>    0.332</td> <td> 0.744</td> <td> -179.698</td> <td>  246.743</td>
</tr>
<tr>
  <th>base</th>   <td>   -0.5393</td> <td>    0.160</td> <td>   -3.378</td> <td> 0.004</td> <td>   -0.876</td> <td>   -0.202</td>
</tr>
<tr>
  <th>chemo</th>  <td>    0.2124</td> <td>    0.103</td> <td>    2.053</td> <td> 0.056</td> <td>   -0.006</td> <td>    0.431</td>
</tr>
<tr>
  <th>rad</th>    <td>   -0.0062</td> <td>    0.031</td> <td>   -0.203</td> <td> 0.841</td> <td>   -0.071</td> <td>    0.059</td>
</tr>
<tr>
  <th>height</th> <td>   -0.2274</td> <td>    0.658</td> <td>   -0.346</td> <td> 0.734</td> <td>   -1.615</td> <td>    1.160</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.589</td> <th>  Durbin-Watson:     </th> <td>   1.812</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.745</td> <th>  Jarque-Bera (JB):  </th> <td>   0.321</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.286</td> <th>  Prob(JB):          </th> <td>   0.852</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.851</td> <th>  Cond. No.          </th> <td>1.46e+04</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.46e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is not a good idea. We end up with a far more complicated model for no appreciable gain in <code>R-squared</code>. The "adjusted $R^2$" penalizes us for using more predictor variables: notice that the value of <code>Adj. R-squared</code> is smaller for the regression with all the predictors than for the regression with just <code>base</code> and <code>chemo</code>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Curious about how to select predictors, or about what makes a good regression? Then take some more statistics classes! This one is complete.</p>

</div>
</div>
</div>
</div>

 

