---
redirect_from:
  - "/notebooks/chapter-12/02-the-distribution-of-the-estimated-slope"
interact_link: content/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope.ipynb
kernel_name: python3
has_widgets: false
title: |-
  The Distribution of the Estimated Slope
prev_page:
  url: /notebooks/Chapter_12/01_The_Simple_Linear_Regression_Model.html
  title: |-
    The Simple Linear Regression Model
next_page:
  url: /notebooks/Chapter_12/03_Towards_Multiple_Regression.html
  title: |-
    Towards Multiple Regression
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Distribution-of-the-Estimated-Slope">The Distribution of the Estimated Slope<a class="anchor-link" href="#The-Distribution-of-the-Estimated-Slope"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our estimate of the signal $Y = \beta_0 + \beta_1x$ is the corresponding point on the regression line:</p>
<p>$$
\hat{Y} ~ = ~ \hat{\beta}_0 + \hat{\beta}_1x
$$</p>
<p>Here $\hat{\beta}_1$ is the slope of the regression line (we called it $\hat{a}$ in our earlier calculations) and $\hat{\beta_0}$ is the intercept (we called it $\hat{b}$) of the regression line.</p>
<p>Recall that when we derived the formula for the best slope $\hat{a}$, we first obtained</p>
<p>$$
\hat{a} ~ = ~ \frac{E(D_XD_Y)}{\sigma_X^2}
$$</p>
<p>where $D_X$ and $D_Y$ were the deviations of $X$ and $Y$. In our current context, we must apply this formula to the empirical distribution of the data. That's easier than it sounds. For example, the variance $\sigma_X^2$ is by definition the mean squared deviation, which becomes $\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2$. The mean product of deviations $E(D_XD_Y)$ becomes $\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Estimated-Slope">Estimated Slope<a class="anchor-link" href="#Estimated-Slope"> </a></h3><p>The least-squares estimate of the true slope $\beta_1$ is the slope of the regression line, given by</p>
<p>$$
\hat{\beta}_1 ~ = ~ \frac{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2}
$$</p>
<p>The average response $\bar{Y}$ is a linear combination of the independent normal random variables $Y_1, Y_2, \ldots, Y_n$. Therefore so is the $i$th deviation $\bar{Y_i} - \bar{Y}$ for each $i$.</p>
<p>Thus $\hat{\beta}_1$ is a linear combination of the independent normal random variables $Y_1, Y_2, \ldots, Y_n$. Therefore the distribution of $\hat{\beta}_1$ is normal.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Expectation-of-the-Estimated-Slope">Expectation of the Estimated Slope<a class="anchor-link" href="#Expectation-of-the-Estimated-Slope"> </a></h3><p>To identify the mean of the distribution of $\hat{\beta}_1$, recall that $E(Y_i) = \beta_0 + \beta_1x_i$ and $E(\bar{Y}) = \beta_0 + \beta_1\bar{x}$. So</p>
<p>$$
E(Y_i - \bar{Y}) ~ = ~ \beta_1(x_i - \bar{x})
$$</p>
<p>Now</p>
<p>$$
\begin{align*}
E(\hat{\beta_1}) ~ &amp;= ~ \frac{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})E(Y_i - \bar{Y})}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \\ \\
&amp;= ~ \frac{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})\beta_1(x_i - \bar{x})}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \\ \\
&amp;= ~ \frac{\beta_1\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \\ \\
&amp;= ~ \beta_1
\end{align*}
$$</p>
<p>We have shown that $\hat{\beta_1}$ is an unbiased estimator of $\beta_1$.</p>
<p>The distribution of the estimated slope $\hat{\beta}_1$ is normal, centered at the true slope $\beta_1$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope_6_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we can find $Var(\hat{\beta}_1)$ then we can use this normal curve for inference. For example,</p>
<p>$$
\hat{\beta}_1 ~ \pm ~ 2SD(\hat{\beta}_1)
$$</p>
<p>is a 95% confidence interval for $\beta_1$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Variance-of-the-Estimated-Slope">Variance of the Estimated Slope<a class="anchor-link" href="#Variance-of-the-Estimated-Slope"> </a></h3><p>To find $Var(\hat{\beta}_1)$ is helps to know how to find variances of sums of dependent random variables, which is outside the scope of this class. So we will simply state that</p>
<p>$$
Var(\hat{\beta}_1) ~ = ~ \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}
$$</p>
<p>In fact we will not use the exact formula for the variance. It is enough to notice that:</p>
<ul>
<li>The numerator is the constant error variance. The denominator gets larger (there are more terms) when $n$ increases. So the more data we have, the closer the estimated slope $\hat{\beta}_1$ will be to the true slope $\beta_1$, most likely.</li>
<li>The expression for $Var(\hat{\beta}_1)$ involves the unknown error variance $\sigma^2$, but no other unknown parameter is involved.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Standard-Error-of-the-Estimated-Slope">Standard Error of the Estimated Slope<a class="anchor-link" href="#Standard-Error-of-the-Estimated-Slope"> </a></h3><p>We know that</p>
<p>$$
SD(\hat{\beta}_1) ~ = ~ \frac{\sigma}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}}
$$</p>
<p>but we can't get a numerical value for this SD since $\sigma$ is unknown.</p>
<p>So we estimate $\sigma$ based on the data, by using the SD of the residuals instead. When the standard deviation of an estimator is estimated from the data, it is sometimes called the <em>standard error</em> of the estimator. We will denote the standard error of $\hat{\beta}_1$ by $SE(\hat{\beta}_1)$.</p>
<p>The larger $n$, the better the estimate of $\sigma$. So for large $n$, the distribution of the standardized slope</p>
<p>$$
T ~ = ~ \frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)}
$$</p>
<p>is approximately standard normal.</p>
<p>Almost all programming languages and statistical systems come with modules or routines that carry out regression calculations. They will provide you with numerical values of $\hat{\beta}_1$ and $SE(\hat{\beta}_1)$. Here is an example.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Pulse-Rates">Pulse Rates<a class="anchor-link" href="#Pulse-Rates"> </a></h3><p>The table <code>pulse</code> from the <a href="https://vincentarelbundock.github.io/Rdatasets/doc/Stat2Data/Pulse.html">R project</a> contains data on the pulse rates of $232$ students before and after exercise. The first two columns are the active and resting pulse rates.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pulse</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Active</th> <th>Rest</th> <th>Smoke</th> <th>Sex</th> <th>Exercise</th> <th>Hgt</th> <th>Wgt</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>97    </td> <td>78  </td> <td>0    </td> <td>1   </td> <td>1       </td> <td>63  </td> <td>119 </td>
        </tr>
        <tr>
            <td>82    </td> <td>68  </td> <td>1    </td> <td>0   </td> <td>3       </td> <td>70  </td> <td>225 </td>
        </tr>
        <tr>
            <td>88    </td> <td>62  </td> <td>0    </td> <td>0   </td> <td>3       </td> <td>72  </td> <td>175 </td>
        </tr>
        <tr>
            <td>106   </td> <td>74  </td> <td>0    </td> <td>0   </td> <td>3       </td> <td>72  </td> <td>170 </td>
        </tr>
        <tr>
            <td>78    </td> <td>63  </td> <td>0    </td> <td>1   </td> <td>3       </td> <td>67  </td> <td>125 </td>
        </tr>
        <tr>
            <td>109   </td> <td>65  </td> <td>0    </td> <td>0   </td> <td>3       </td> <td>74  </td> <td>188 </td>
        </tr>
        <tr>
            <td>66    </td> <td>43  </td> <td>0    </td> <td>1   </td> <td>3       </td> <td>67  </td> <td>140 </td>
        </tr>
        <tr>
            <td>68    </td> <td>65  </td> <td>0    </td> <td>0   </td> <td>3       </td> <td>70  </td> <td>200 </td>
        </tr>
        <tr>
            <td>100   </td> <td>63  </td> <td>0    </td> <td>0   </td> <td>1       </td> <td>70  </td> <td>165 </td>
        </tr>
        <tr>
            <td>70    </td> <td>59  </td> <td>0    </td> <td>1   </td> <td>2       </td> <td>65  </td> <td>115 </td>
        </tr>
    </tbody>
</table>
<p>... (222 rows omitted)</p>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our goal is to predict the active pulse rate based on the resting pulse rate.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pulse</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;Rest&#39;</span><span class="p">,</span> <span class="s1">&#39;Active&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope_14_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The scatter plot of the active rate versus the resting rate shows a linear relation. The plot is a bit wider in the middle than it is elsewhere, which indicates that the regression model's assumption of equal error variances for all individuals might not be satisfied. However, we can use the model as a rough approximation of reality.</p>
<p>To use Python to perform the regression, we must first get the data in an appropriate form. One way is to extract the values of the response and predictor variables as arrays.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">active</span> <span class="o">=</span> <span class="n">pulse</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">resting</span> <span class="o">=</span> <span class="n">pulse</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The call <code>stats.linregress(x, y)</code> performs the simple linear regression of the array <code>y</code> on the array <code>x</code>. That means $y$ is the response and $x$ the predictor variable.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">linregress</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">resting</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">active</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>LinregressResult(slope=1.142879681904831, intercept=13.182572776013345, rvalue=0.6041870881060092, pvalue=1.7861044071652305e-24, stderr=0.09938884436389145)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's a bit of a mess. Let's assign each element of the output to a name. Note that <code>stderr</code> is the SE of the slope.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">se_slope</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">linregress</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">resting</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">active</span><span class="p">)</span>
<span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">se_slope</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(1.142879681904831,
 13.182572776013345,
 0.6041870881060092,
 1.7861044071652305e-24,
 0.09938884436389145)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The numbers in the output are the observed values of:</p>
<ul>
<li>the estimated slope $\hat{\beta}_1$</li>
<li>the estimated intercept $\hat{\beta_0}$</li>
<li>the correlation $r$</li>
<li>the $p$-value of a test that we will describe below</li>
<li>the standard error of the estimated slope, $SE(\hat{\beta}_1)$.</li>
</ul>
<p>The sample size is large ($232$), so the distribution of $\hat{\beta}_1$ is approximately normal with mean $\beta_1$ and SD approximately equal to $SE(\hat{\beta}_1)$. Therefore the calculation below results in an approximate 95% confidence interval for the true slope $\beta_1$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Approximate 95% confidence interval for the true slope</span>

<span class="n">slope</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">se_slope</span><span class="p">,</span> <span class="n">slope</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">se_slope</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(0.9441019931770481, 1.341657370632614)</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A fundamentally important question is whether the true slope $\beta_1$ is $0$. If it is $0$ then the resting pulse rate isn't involved in the prediction of the active pulse rate, according to the regression model.</p>
<p>The 95% confidence interval for the slope doesn't contain 0, so we can conclude (at the 5% level) that the true slope isn't 0. We can also carry out a formal test of hypotheses as follows.</p>
<ul>
<li>$H_0$: $\beta_1 = 0$</li>
<li>$H_A$: $\beta_1 \ne 0$</li>
</ul>
<p>Under the null hypothesis, the distribution of the standardized slope</p>
<p>$$
T ~ = ~ \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)}
$$</p>
<p>is approximately standard normal. The observed value of the test statistic is $11.5$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># observed statistic under H_0:</span>

<span class="p">(</span><span class="n">slope</span> <span class="o">-</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">se_slope</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>11.499074058255635</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The observed slope is $11.5$ standard errors above the expected slope of $0$ under the null hypothesis. That's a whole lot of standard errors away from expectation. The data are not consistent with $H_0$; they favor the alternative hypothesis that the true slope is not $0$.</p>
<p>The output of <code>stats.linregress</code> provides the $p$-value of this test. Not surprisingly, it is microscopically small.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># p-value</span>

<span class="n">p</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1.7861044071652305e-24</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$t$-Statistic">$t$ Statistic<a class="anchor-link" href="#$t$-Statistic"> </a></h3><p>The calculation above relies on the sample size being large enough so that $SE(\hat{\beta}_1)$ is almost certain to be essentially equal to $SD(\hat{\beta}_1)$. When the sample size is small or moderate, this might not be a good assumption.</p>
<p>Fortunately, it turns out that it is possible to identify the exact distribution of $T$ for any sample size $n$. It is one of a family of bell-shaped distributions called the <em>$t$ distributions</em>.</p>
<p>All $t$ distributions are symmetric around 0, just as the standard normal distribution is.</p>
<p>The family of $t$ distributions is indexed by the positive integers: there's the $t$-distribution Number 1, the $t$-distribution Number 2, and so on. For reasons we won't go into, statisticians refer to the index as the <em>degrees of freedom</em> of the distribution.</p>
<p>The $t$ density looks like the standard normal curve, except that it has fatter tails. In the figure below, the red curve is the standard normal density and the blue curve is the $t$ density with $3$ degrees of freedom.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope_28_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can see that the $t$ distribution has a bit more weight in the tails than the standard normal does. It gives us just the right additional amount of wiggle room to account for the fact that we are estimating the unknown error variance $\sigma^2$ based on the data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When the degrees of freedom are larger then the $t$ distributions are almost indistinguishable from the standard normal as you can see below. So it's fine to use normal approximations the estimated slope when the sample size is large.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope_31_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For reasons we won't go into, under the regression model the exact distribution of the standardized slope $T$ is $t$ Number $n-2$ when the sample size is $n$. That is, for any sample size $n$, the standardized slope</p>
<p>$$
T ~ = ~ \frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)}
$$</p>
<p>has the $t$ distribution with $n-2$ degrees of freedom.</p>
<p>When the sample size $n$ is large, so is $n-2$, so we might as well use the normal curve. When the sample is size is small, using the appropriate $t$ curve gives more accurate answers.</p>
<p>To remember the number of degrees of freedom, keep in mind that we are basing our estimate of the response $Y$ on the two estimated parameters $\hat{\beta}_0$ and $\hat{\beta}_1$. Start with degrees of freedom equal to the sample size, and then lose one degree of freedom each for the intercept and slope. This rule can be extended to the case of multiple regression, as we will see in the next section.</p>

</div>
</div>
</div>
</div>

 

