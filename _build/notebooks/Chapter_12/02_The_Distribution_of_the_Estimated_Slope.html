---
redirect_from:
  - "/notebooks/chapter-12/02-the-distribution-of-the-estimated-slope"
interact_link: content/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope.ipynb
kernel_name: python3
has_widgets: false
title: |-
  The Distribution of the Estimated Slope
prev_page:
  url: /notebooks/Chapter_12/01_The_Simple_Linear_Regression_Model.html
  title: |-
    The Simple Linear Regression Model
next_page:
  url: 
  title: |-
    
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Distribution-of-the-Estimated-Slope">The Distribution of the Estimated Slope<a class="anchor-link" href="#The-Distribution-of-the-Estimated-Slope"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our estimate of the signal $Y = \beta_0 + \beta_1x$ is the corresponding point on the regression line:</p>
<p>$$
\hat{Y} ~ = ~ \hat{\beta}_0 + \hat{\beta}_1x
$$</p>
<p>Here $\hat{\beta}_1$ is the slope of the regression line (we called it $\hat{a}$ in our earlier calculations) and $\hat{\beta_0}$ is the intercept (we called it $\hat{b}$) of the regression line.</p>
<p>Recall that when we derived the formula for the best slope $\hat{a}$, we first obtained</p>
<p>$$
\hat{a} ~ = ~ \frac{E(D_XD_Y)}{\sigma_X^2}
$$</p>
<p>where $D_X$ and $D_Y$ were the deviations of $X$ and $Y$. In our current context, we must apply this formula to the empirical distribution of the data. That's easier than it sounds. For example, the variance $\sigma_X^2$ is by definition the mean squared deviation, which becomes $\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2$. The mean product of deviations $E(D_XD_Y)$ becomes $\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Estimated-Slope">Estimated Slope<a class="anchor-link" href="#Estimated-Slope"> </a></h3><p>The least-squares estimate of the true slope $\beta_1$ is the slope of the regression line, given by</p>
<p>$$
\hat{\beta}_1 ~ = ~ \frac{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2}
$$</p>
<p>The average response $\bar{Y}$ is a linear combination of the independent normal random variables $Y_1, Y_2, \ldots, Y_n$. Therefore so is the $i$th deviation $\bar{Y_i} - \bar{Y}$ for each $i$.</p>
<p>Thus $\hat{\beta}_1$ is a linear combination of the independent normal random variables $Y_1, Y_2, \ldots, Y_n$. Therefore the distribution of $\hat{\beta}_1$ is normal.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Expectation-of-the-Estimated-Slope">Expectation of the Estimated Slope<a class="anchor-link" href="#Expectation-of-the-Estimated-Slope"> </a></h3><p>To identify the mean of the distribution of $\hat{\beta}_1$, recall that $E(Y_i) = \beta_0 + \beta_1x_i$ and $E(\bar{Y}) = \beta_0 + \beta_1\bar{x}$. So</p>
<p>$$
E(Y_i - \bar{Y}) ~ = ~ \beta_1(x_i - \bar{x})
$$</p>
<p>Now</p>
<p>$$
\begin{align*}
E(\hat{\beta_1}) ~ &amp;= ~ \frac{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})E(Y_i - \bar{Y})}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \\ \\
&amp;= ~ \frac{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})\beta_1(x_i - \bar{x})}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \\ \\
&amp;= ~ \frac{\beta_1\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \\ \\
&amp;= ~ \beta_1
\end{align*}
$$</p>
<p>We have shown that $\hat{\beta_1}$ is an unbiased estimator of $\beta_1$.</p>
<p>The distribution of the estimated slope $\hat{\beta}_1$ is normal, centered at the true slope $\beta_1$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope_5_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we can find $Var(\hat{\beta}_1)$ then we can use this normal curve for inference. For example,</p>
<p>$$
\hat{\beta}_1 ~ \pm ~ 2SD(\hat{\beta}_1)
$$</p>
<p>is an approximate 95% confidence interval for $\beta_1$.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Variance-of-the-Estimated-Slope">Variance of the Estimated Slope<a class="anchor-link" href="#Variance-of-the-Estimated-Slope"> </a></h3><p>To find $Var(\hat{\beta}_1)$ is helps to know how to find variances of sums of dependent random variables, which is outside the scope of this class. So we will simply state that</p>
<p>$$
Var(\hat{\beta}_1) ~ = ~ \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}
$$</p>
<p>In fact we will not use the exact formula for the variance. It is enough to notice that:</p>
<ul>
<li>The numerator is the constant error variance. The denominator gets larger (there are more terms) when $n$ increases. So the more data we have, the closer the estimated slope $\hat{\beta}_1$ will be to the true slope $\beta_1$, most likely.</li>
<li>The expression for $Var(\hat{\beta}_1)$ involves the unknown error variance $\sigma^2$, but no other unknown parameter is involved.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Standard-Error-of-the-Estimated-Slope">Standard Error of the Estimated Slope<a class="anchor-link" href="#Standard-Error-of-the-Estimated-Slope"> </a></h3><p>We know that</p>
<p>$$
SD(\hat{\beta}_1) ~ = ~ \frac{\sigma}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}}
$$</p>
<p>but we can't get a numerical value for this SD since $\sigma$ is unknown.</p>
<p>So we estimate $\sigma$ based on the data, by using the SD of the residuals instead. When the standard deviation of an estimator is estimated from the data, it is sometimes called the <em>standard error</em> of the estimator. We will denote the standard error of $\hat{\beta}_1$ by $SE(\hat{\beta}_1)$.</p>
<p>The larger $n$, the better the estimate of $\sigma$. So for large $n$, the distribution of the standardized slope</p>
<p>$$
T ~ = ~ \frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)}
$$</p>
<p>is approximately normal.</p>
<p>Almost all programming languages and statistical systems come with modules or routines that carry out regression calculations. They will provide you with numerical values of $\hat{\beta}_1$ and $SE(\hat{\beta}_1)$. We will see some examples in the next section.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="$t$-Statistic">$t$ Statistic<a class="anchor-link" href="#$t$-Statistic"> </a></h3><p>It turns out that it is possible to identify the exact distribution of $T$ for any sample size $n$. It is one of a family of bell-shaped distributions called the <em>$t$ distributions</em>.</p>
<p>All $t$ distributions are symmetric around 0, just as the standard normal distribution is.</p>
<p>The family of $t$ distributions is indexed by the positive integers: there's the $t$-distribution Number 1, the $t$-distribution Number 2, and so on. For reasons we won't go into, statisticians refer to the index as the <em>degrees of freedom</em> of the distribution.</p>
<p>Also for reasons we won't go into, the exact distribution of the standardized slope $T$ is $t$ Number $n-2$. That is, for all $n$, the standardized slope</p>
<p>$$
T ~ = ~ \frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)}
$$</p>
<p>has the $t$ distribution with $n-2$ degrees of freedom.</p>
<p>To remember the number of degrees of freedom, remember that we are basing our estimate of the response $Y$ on the two estimated parameters $\hat{\beta}_0$ and $\hat{\beta}_1$. Start with degrees of freedom equal to the sample size, and then lose one degree of freedom each for the intercept and slope. This rule can be extended to the case of multiple regression, as we will see in the next section.</p>
<p>The $t$ density looks like the standard normal curve, except that it has fatter tails. In the figure below, the red curve is the standard normal density and the blue curve is the $t$ density with $4$ degrees of freedom.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope_10_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can see that the $t$ distribution has a bit more weight in the tails than the standard normal does. It gives us just the right additional amount of wiggle room to account for the fact that we are estimating the unknown error variance $\sigma^2$ from the data.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When the degrees of freedom are larger (corresponding to larger sample sizes in our case), then the $t$ distributions are almost indistinguishable from the standard normal as you can see below. So it's fine to use normal approximations the estimated slope when the sample size is large.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope_13_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the next section (the final one in this book!) we will use Python and the theory we have developed to carry out some regression analyses.</p>

</div>
</div>
</div>
</div>

 

