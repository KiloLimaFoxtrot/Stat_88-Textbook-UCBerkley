---
redirect_from:
  - "/notebooks/chapter-09/02-ab-testing-fishers-exact-test"
interact_link: content/notebooks/Chapter_09/02_AB_Testing_Fishers_Exact_Test.ipynb
kernel_name: python3
has_widgets: false
title: |-
  A/B Testing: Fisher's Exact Test
prev_page:
  url: /notebooks/Chapter_09/01_Testing_Hypotheses.html
  title: |-
    Testing Hypotheses
next_page:
  url: /notebooks/Chapter_09/03_Confidence_Intervals_Method.html
  title: |-
    Confidence Intervals Method
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A/B-Testing:-Fisher's-Exact-Test">A/B Testing: Fisher's Exact Test<a class="anchor-link" href="#A/B-Testing:-Fisher's-Exact-Test"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>A/B testing</em> is a term used to describe tests of hypotheses that involve comparing the distributions of two random samples. Although the term is relatively new, the ideas and methods have been used by statisticians for a very long time.</p>
<p>In this section we revisit a test of hypotheses performed in Data 8 using random permutations. We will show how the test can be performed using a method devised by Sir Ronald Fisher early in the 20th century.</p>
<p>Recall a <a href="https://www.inferentialthinking.com/chapters/12/3/Causality.html">randomized controlled experiment</a> to study a potential treatment for chronic back pain. The treatment was the botulinum toxin A, a very potent toxin that can be used as medication in tiny doses.</p>
<p>A total of 31 patients participated in the study. Of these, 15 were randomly assigned to the treatment group and the remaining 16 to the control group. Eight weeks after treatment, 11 of the 15 in the treatment group had pain relief compared to 2 out of 16 in the control group.</p>
<p>In other words, of the 13 patients who had pain relief, 11 were in the treatment group and 2 in the control group. Is this evidence in favor of the treatment?</p>
<p>We can answer this question by performing a test of hypotheses.</p>
<p>The null hypothesis $H_0$ says that the treatment does nothing; any difference between the two groups is due to the random assignment of patients to treatment and control.</p>
<p>The alternative hypothesis $H_A$ says that the treatment was beneficial.</p>
<p>A natural test statistic to use is the number of treated patients who had pain relief. Call this statistic $X$. High values of $X$ favor the alternative hypothesis.</p>
<p>Under $H_0$, the treatment did nothing. So among the 31 patients in the study, 13 would have had pain relief anyway, regardless of the assignment to groups. The only reason 11 of them ended up in the treatment group is due to the random assignment of patients to groups.</p>
<p>Thus the $p$-value is the chance of getting 11 or more of the 13 "pain relief" patients in the treatment group, just by assigning 15 randomly picked patients to the treatment group.</p>
<p>This is a hypergeometric probability. The parameters are</p>
<ul>
<li>N = 31, the population size</li>
<li>G = 13, the total number of "pain relief" patients</li>
<li>n = 15, the size of the treatment group</li>
</ul>
<p>The $p$-value is</p>
<p>$$
P_{H_0}(X \ge 11) ~ = ~ \sum_{g=11}^{13} \frac{\binom{13}{g}\binom{18}{15-g}}{\binom{31}{15}} ~ \approx ~ 0.0008
$$</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">hypergeom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">make_array</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.0008299755046076295</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's a very small $p$-value, which implies that the data support the alternative hypothesis more than they support the null. The treatment helped. This is consistent with the conclusion of the researchers and also with our own analysis in Data 8. But all three analyses are different.</p>
<ul>
<li>The calculations in the research paper involve some approximations.</li>
<li>In Data 8 we simulated the difference between the two group proportions under the null hypothesis, by pooling the two groups and randomly permuting the pooled sample. Our conclusion was based on an empirical, approximate $p$-value.</li>
<li>The calculation here does not require simulation and produces an exact $p$-value.</li>
</ul>
<p>The method we have used is called <em>Fisher's exact test</em>. That's the same Sir Ronald Fisher who formalized tests of hypotheses, suggested cutoffs for $p$-values, and so on. The method can be used for any sample size and any randomized controlled experiment with a binary response.</p>

</div>
</div>
</div>
</div>

 

