---
redirect_from:
  - "/notebooks/chapter-07/02-sampling-without-replacement"
interact_link: content/notebooks/Chapter_07/02_Sampling_Without_Replacement.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Sampling Without Replacement
prev_page:
  url: /notebooks/Chapter_07/01_Sums_of_Independent_Random_Variables.html
  title: |-
    Sums of Independent Random Variables
next_page:
  url: /notebooks/Chapter_07/03_The_Law_of_Averages.html
  title: |-
    The Law of Averages
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sampling-Without-Replacement">Sampling Without Replacement<a class="anchor-link" href="#Sampling-Without-Replacement"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The draws in a simple random sample aren't independent of each other. This makes calculating variances a little less straightforward than in the case of draws with replacement.</p>
<p>In this section we will find the variance of a random variable that has a hypergeometric distribution. Then we will use the variance to examine the accuracy of polls.</p>
<p>As a preliminary, let's do some calculations involving indicator random variables.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Squares-and-Products-of-Indicators">Squares and Products of Indicators<a class="anchor-link" href="#Squares-and-Products-of-Indicators"> </a></h3><p>Let $I_A$ be the indicator of the event $A$. Then the distribution of $I_A$ is given by</p>
<table>
<thead><tr>
<th style="text-align:right">value</th>
<th style="text-align:center">$0$</th>
<th style="text-align:center">$1$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right"><strong>probability</strong></td>
<td style="text-align:center">$1-P(A)$</td>
<td style="text-align:center">$P(A)$</td>
</tr>
</tbody>
</table>
<p>We know that $E(I_A) = P(A)$.</p>
<p>Now let's look at the random variable $I_A^2$. As we have seen before, this is a function of $I_A$ that is in fact equal to $I_A$: when $I_A = 0$ then $I_A^2 = 0$ and when $I_A = 1$ then $I_A^2 = 1$.</p>
<p>So $I_A^2 = I_A$ and hence $E(I_A^2) = P(A)$.</p>
<p>Now let $I_B$ be the indicator of the event $B$, and consider the product $I_AI_B$.</p>
<p>This product is itself an indicator. It has the value 1 when both A and B occur, and it is 0 otherwise. In other words, it is the indcator of the event $AB$.</p>
<p>Therefore $E(I_AI_B) ~ = ~ P(AB)$.</p>
<p>The expectation of the product of two indicators is the probability that both the events being indicated occur.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="SD-of-the-Hypergeometric">SD of the Hypergeometric<a class="anchor-link" href="#SD-of-the-Hypergeometric"> </a></h3><p>Let $X$ have the hypergeometric $(N, G, n)$ distribution. That is, let $X$ be the number of good elements in a simple random sample of size $n$ drawn from a population of $N$ elements of which $G$ are good.</p>
<p>As we have seen before,</p>
<p>$$
X ~ = ~ I_1 + I_2 + \cdots + I_n
$$</p>
<p>where $I_j$ is the indicator of the event that the $j$th draw is good.</p>
<p>We know that $I_1, I_2, \ldots, I_n$ are identically distributed, and that each has chance $G/N$ of being 1. That was how we showed that</p>
<p>$$
E(X) ~ = ~ n\frac{G}{N}
$$</p>
<p>But we can't just add the variances of the indicators to get the variance of $X$. Unlike the binomial case, these indicators aren't independent.</p>
<p>So let's get back to basics and try to use</p>
<p>$$
Var(X) ~ = ~ E(X^2) - (E(X)^2) ~ = ~ E(X^2) - \big{(} n\frac{G}{N} \big{)}^2
$$</p>
<p>To find $E(X^2)$ note that</p>
<p>$$
X^2 ~ = ~ (I_1 + I_2 + \cdots + I_n)^2 ~ = ~ \sum_{j=1}^n I_j^2 + \mathop{\sum_{j=1}^n \sum_{k=1}^n}_{j \ne k} I_jI_k
$$</p>
<p>So</p>
<p>$$
\begin{align*}
E(X^2) ~ &amp;= ~ E(\sum_{j=1}^n I_j^2) + E(\mathop{\sum_{j=1}^n \sum_{k=1}^n}_{j \ne k} I_jI_k)\\
&amp;=~ \sum_{j=1}^n E(I_j^2) + \mathop{\sum_{j=1}^n \sum_{k=1}^n}_{j \ne k} E(I_jI_k)\\
&amp;=~ nE(I_1^2) + n(n-1)E(I_1I_2)
\end{align*}
$$</p>
<p>by the symmetry of simple random sampling.</p>
<p>Apply our results about indicators to see that</p>
<p>$$
E(X^2) ~ = ~ n \frac{G}{N} + n(n-1)\frac{G}{N}\cdot\frac{G-1}{N-1}
$$</p>
<p>and therefore</p>
<p>$$
Var(X) ~ = ~ n \frac{G}{N} + n(n-1)\frac{G}{N}\cdot\frac{G-1}{N-1} - \big{(} n\frac{G}{N} \big{)}^2
$$</p>
<p>That looks awful but it's actually not so bad. Pull out a common factor:</p>
<p>$$
Var(X) ~ = ~ n\frac{G}{N}\big{(} 1 + (n-1)\frac{G-1}{N-1} - n\frac{G}{N}\big{)}
$$</p>
<p>After a little manipulation this becomes</p>
<p>$$
Var(X) ~ = ~ n\frac{G}{N} \cdot \frac{N-G}{N}\cdot\frac{N-n}{N-1}
$$</p>
<p>The initial part of this formula is the binomial variance $npq$. To see this more clearly, write $B = N-G$ for the number of bad elements. Then</p>
<p>$$
Var(X) ~ = ~ \big{(} n\frac{G}{N} \cdot \frac{B}{N}\big{)}\frac{N-n}{N-1} 
$$</p>
<p>and</p>
<p>$$
SD(X) ~ = ~ \sqrt{n\frac{G}{N} \cdot \frac{B}{N}}\sqrt{\frac{N-n}{N-1}} ~ = ~ \sqrt{npq} \sqrt{\frac{N-n}{N-1}}
$$</p>
<p>for $p = \frac{G}{N}$.</p>
<p>For example, the number of hearts in a 5-card poker hand is expected to be</p>
<p>$$
5 \times \frac{13}{52} ~ \approx ~ 1.25
$$</p>
<p>with an SD of</p>
<p>$$
\sqrt{5 \times \frac{13}{52} \times \frac{39}{52}}\sqrt{\frac{52 - 5}{52 - 1}} ~ \approx ~ 0.93
$$</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Size-of-the-FPC">The Size of the FPC<a class="anchor-link" href="#The-Size-of-the-FPC"> </a></h3><p>We have shown that the SD of the number of good elements when drawing <em>without</em> replacement is the same as though we had been drawing <em>with</em> replacement, times the <em>finite population correction</em> or fpc given by</p>
<p>$$
\text{fpc} ~ = ~ \sqrt{\frac{N-n}{N-1}}
$$</p>
<p>Since the sample size is typically greater than 1, the fpc is typically less than 1.</p>
<p>This implies that the hypergeoemtric SD is <em>less</em> than the corresponding binomial SD if the draws had been made with replacement.</p>
<p>The figure below shows the histogram of the hypergeometric $(52, 26, 5)$ distribution (the distribution of the number of red cards in a poker hand) and the binomial $(5, 26/52)$ distribution.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell tag_remove_input">

<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/notebooks/Chapter_07/02_Sampling_Without_Replacement_6_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Both histograms are centered at 2.5. You can see that the hypergeometric histogram is a bit taller and hence a bit less spread out than the binomial.</p>
<p>But not by much! Let's see why.</p>
<p>As we have observed before, sampling with and without replacement are essentially the same when the sample size is small relative to the population size. We now have another confirmation of this.</p>
<p>When the sample size is small relative to the population, the finite population correction is close to 1. That is because</p>
<p>$$
\frac{N-n}{N-1} ~ = ~ 1 - \frac{n-1}{N-1} ~ \approx ~ 1 - \frac{n}{N} ~ \approx ~ 1
$$</p>
<p>when $\frac{n}{N}$ is small.</p>
<p>The SD of the number of good elements in the sample is the same for sampling with and without replacement, apart from the fpc. When the fpc is close to 1, the two SDs are essentially equal.</p>
<p>Data scientists often have to work with relatively small samples drawn from large populations. We see once more that in such situations they can treat the draws as if they had been made with replacement.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Accuracy-of-Simple-Random-Samples">The Accuracy of Simple Random Samples<a class="anchor-link" href="#The-Accuracy-of-Simple-Random-Samples"> </a></h3><p>When politicans don't like the result of a poll, they sometimes say they don't trust polls that are based on tiny percents of the population. Let's see what we think of that.</p>
<p>Suppose a poll is based on a simple random sample drawn from a huge population of voters of whom a proportion $p$ favor a politician. Then the SD of the number of voters who favor the politician is</p>
<p>$$
\sqrt{npq}\sqrt{\frac{N-n}{N-1}} ~ \approx ~ \sqrt{npq}
$$</p>
<p>because the fpc is close to 1.</p>
<p>Essentially, the SD depends on $p$ and on the <em>sample</em> size $n$, not on the population size $N$. The population size appears only in the fpc which is close to 1. In effect, the population size is so large in comparison to the sample size that it might as well be infinite.</p>
<p>Thus if all other things (such as $p$) are equal, a simple random sample of size 100 taken from Berkeley (population about 120,000) is just about as accurate as a simple random sample of size 100 taken from San Francisco (population about 880,000).</p>
<p>As for the doubting politicians, we have to remind them that when the population is large, it's the sample size that matters, not the population size. A a 0.1% sample of one million voters is a small percent of the population but it consists of $n = 1000$ voters. Simple random samples that big are pretty accurate, as we will see in the next section.</p>

</div>
</div>
</div>
</div>

 

