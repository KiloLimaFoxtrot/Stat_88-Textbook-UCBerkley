---
redirect_from:
  - "/notebooks/chapter-12/01-the-simple-linear-regression-model"
interact_link: content/notebooks/Chapter_12/01_The_Simple_Linear_Regression_Model.ipynb
kernel_name: python3
has_widgets: false
title: |-
  The Simple Linear Regression Model
prev_page:
  url: /notebooks/Chapter_12/00_Inference_in_Regression.html
  title: |-
    Inference in Regression
next_page:
  url: /notebooks/Chapter_12/02_The_Distribution_of_the_Estimated_Slope.html
  title: |-
    The Distribution of the Estimated Slope
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Simple-Linear-Regression-Model">The Simple Linear Regression Model<a class="anchor-link" href="#The-Simple-Linear-Regression-Model"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The model involves a variable called the <em>response</em> and another called a <em>predictor variable</em> or <em>feature</em>. The assumptions are that the response is a linear function of the predictor variable, called a <em>signal</em>, plus random error, called <em>noise</em>:</p>
<p>$$
\text{response} ~ = ~ \text{signal} ~ + ~ \text{noise}
$$</p>
<p>Our job is to extract the signal. The data are observations on the predictor variable and the response of $n$ individuals. We assume that we can measure the predictor variable perfectly accurately, and that it is therefore not random. The only random component of the response is the noise.</p>
<p>Formally, for individuals $i = 1, 2, 3, \ldots, n$, the response is assumed to be</p>
<p>$$
Y_i ~ = ~ \beta_0 + \beta_1x_i + \epsilon_i
$$</p>
<p>where:</p>
<ul>
<li>$\beta_0$ and $\beta_1$ are unobservable constant parameters.</li>
<li>$x_i$ is the value of the predictor variable for individual $i$ and is assumed to be constant (that is, not random).</li>
<li>The errors $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ are i.i.d. normal $(0, \sigma^2)$ random variables.</li>
<li>The error variance $\sigma^2$ is an unobservable constant parameter, and is assumed to be the same for all individuals $i$.</li>
</ul>
<p>For each individual $i$, we would like to get as close as we can to the signal $\beta_0 + \beta_1x_i$. We can't see the true line $y = \beta_0 + \beta_1x$, but we can estimate it by the regression line. Thus our estimator of the $i$th signal $\beta_0 + \beta_1x_i$ is</p>
<p>$$
\hat{Y}_i ~ = ~ \hat{\beta}_0 + \hat{\beta}_1x_i
$$</p>
<p>where $\hat{\beta}_0$ and $\hat{\beta}_1$ are the interecept and slope of the regression line.</p>
<p>To estimate the true slope $\beta_1$, it is a good idea to examine the properties of the estimated slope $\hat{\beta}_1$. Before that, let's examine the response.</p>
<p>It is important to keep in mind that all the subsequent calculations in this chapter will be performed under the regression model. The model does not pretend to be the truth: is only a set of assumptions about reality. If the assumptions are not valid for your data, then the calculations of this chapter will not be valid either.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Individual-Responses">Individual Responses<a class="anchor-link" href="#Individual-Responses"> </a></h3><p>Fix $i$ in the range $1$ through $n$. The response $Y_i$ of individual $i$ is the sum of two pieces: the $i$th signal $\beta_0 + \beta_1x_i$ and the noise $\epsilon_i$.</p>
<p>The signal is a constant, and $\epsilon_i$ has the normal $(0, \sigma^2)$ distribution.</p>
<p>Therefore <strong>the response $Y_i$ of individual $i$ has the normal distribution with mean $\beta_0 + \beta_1x_i$ and variance $\sigma^2$</strong>.</p>
<p>The randomness in $Y_i$ comes only from $\epsilon_i$ as the $i$th signal is a constant. Since $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ are independent, the individual responses $Y_1, Y_2, \ldots, Y_n$ are independent of each other.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Average-Response">Average Response<a class="anchor-link" href="#Average-Response"> </a></h3><p>Let $\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$ denote the average response of the $n$ individuals.</p>
<p>Then $\bar{Y}$ is a linear combination of independent normally distributed random variables. So the distribution of $\bar{Y}$ is normal.</p>
<p>The mean of the distribution is</p>
<p>$$
\begin{align*}
E(\bar{Y}) ~ &amp;= ~ \frac{1}{n}\sum_{i=1}^nE(Y_i) \\
&amp;= ~ \frac{1}{n}\sum_{i=1}^n (\beta_0 + \beta_1 x_i) \\
&amp;= ~ \frac{1}{n}\sum_{i=1}^n \beta_0 ~ + ~ \frac{1}{n}\sum_{i=1}^n \beta_1x_i \\
&amp;= ~ \frac{1}{n} \cdot n\beta_0 ~ + ~ \beta_1\frac{1}{n}\sum_{i=1}^n x_i \\
&amp;= ~ \beta_0 + \beta_1 \bar{x}
\end{align*}
$$</p>
<p>Thus the expected average response is the signal at the average value of the predictor variable.</p>
<p>Since the individual responses are independent of each other, we have</p>
<p>$$
Var(\bar{Y}) ~ = ~ \frac{1}{n^2}\sum_{i=1}^n Var(Y_i) ~ = ~ \frac{1}{n^2}\cdot n\sigma^2 ~ = ~ \frac{\sigma^2}{n}
$$</p>
<p>To summarize, <strong>the distribution of the average response $\bar{Y}$ is normal with mean $\beta_0 + \beta_1\bar{x}$ and variance $\frac{\sigma^2}{n}$</strong>.</p>

</div>
</div>
</div>
</div>

 

